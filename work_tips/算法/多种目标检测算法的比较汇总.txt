1. R-CNN :　速度慢,准确度不高,每一张图片需要缩放成　固定尺寸图片进行特征提取
    1.先是由 SS(选择性搜索SelectiveSearch)算法提取区域建议框,
    2.提取图像由SS算法的候选框进行特征提取,
    3.训练一个SVM分类器进行类别分类
    4.使用回归算法修正框的位置


2.SPP-net :　优点:只对原图进行一次卷积得到整张图的feature map,然后找到每个候选框在feature map上的映射patch
                将此patch作为每个候选框的卷积特征输入到SPP layer(ROI　pooling 兴趣区域池化)和之后的层.节省了大量的计算时间,比R-CNN有一百倍左右的提速
    1.依然是由SS(SelectiveSearch选择性搜索)算法提取候选框
    2.原图片经过卷积层得到feature_map,找到候选框在feature_map上的映射特征patch
    3.将patch特征通过ROI pooling统一成(16+4+1)c的固定维度特征
    4.放入之后的层进行处理,最后与R-CNN一样是使用SVM分类和bbox回归


3.fast-r-cnn : 优点:使用了 SPP-net的 ROI pooling进行特征提取,使用网络进行对输入的图片特征进行分类与回归
               缺点:依然是采用的 SS(选择性搜索)算法　进行候选框提取,相当耗时
    1.由SS(SelectiveSearch选择性搜索)算法提取候选框　(预操作)
    2.对整张进行卷积操作得到feature_map,并根据候选框找到候选框在feature_map(conv_5)上的映射特征patch
    3.patch经过　ROI pooling操作后进行全连接层处理　最后使用softmax分类以及得到框回归


4.faster-R-CNN : 优点:　使用 RPN进行候选框提取　与　fast-r-cnn网络交替训练(两个网络都有输出)
    1.输入一张原图进行卷积操作得到feature_map
    2.在最后一个卷积层后加入RPN,　RPN经过训练直接输出候选框,并且每一个候选框都有一个score
    3.对候选框进行筛选处理
    4.把处理完的候选框进行特征提取,进行ROI pooling得到256-d的特征,最后通过全连接层进行分类(softmax)与回归(bbox_pred)
    
    下面着重介绍 RPN 以及　网络训练过程:
        RPN : • 在feature map上滑动窗口
            　　• 建一个神经网络用于物体分类+框位置的回归
            　　• 滑动窗口的位置提供了物体的大体位置信息
            　　• 框的回归提供了框更精确的位置、

            其中注意的是 RPN 网络有2个输出,一个是锚点anchor中有待检测物体的概率,一个是框回归bbox的进一步精确位置

        anchors: 锚点
            anchor的本质是什么，本质是SPP(spatial pyramid pooling)思想的逆向。而SPP本身是做什么的呢，就是将不同尺寸的输入resize成为相同尺寸的输出。所以SPP的逆向就是，将相同尺寸的输出，倒推得到不同尺寸的输入。
            接下来是anchor的窗口尺寸，这个不难理解，三个面积尺寸（128^2，256^2，512^2），然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）.这样一来，我们得到了一共9种面积尺寸各异的anchor。

            假设输入图片(224*224*3)经过一系列卷积和relu操作得到 51*39*256的 feature_map
            在这个特征参数的基础上，通过一个3x3的滑动窗口，在这个51x39的区域上进行滑动，stride=1，padding=2，这样一来，滑动得到的就是51x39个3x3的窗口。

            对于每个3x3的窗口，作者就计算这个滑动窗口的中心点所对应的原始图片的中心点.然后作者假定,这个3x3窗口，是从原始图片上通过SPP池化得到的，而这个池化的区域的面积以及比例，就是一个个的anchor。
            换句话说，对于每个3x3窗口，作者假定它来自9种不同原始区域的池化，但是这些池化在原始图片中的中心点，都完全一样。这个中心点，就是刚才提到的，3x3窗口中心点所对应的原始图片中的中心点。
            如此一来，在每个窗口位置，我们都可以根据9个不同长宽比例、不同面积的anchor，逆向推导出它所对应的原始图片中的一个区域，这个区域的尺寸以及坐标，都是已知的。而这个区域，就是我们想要的 proposal。
            所以我们通过滑动窗口和anchor，成功得到了 51x39x9 个原始图片的proposal。接下来，每个proposal我们只输出6个参数：
            每个 proposal 和 ground truth 进行比较得到的前景概率和背景概率(2个参数）（对应图上的 cls_score）；由于每个 proposal 和 ground truth 位置及尺寸上的差异，从 proposal 通过平移放缩得到 ground truth 需要的4个平移放缩参数（对应图上的 bbox_pred）。
            所以根据我们刚才的计算，我们一共得到了多少个anchor box呢？
            51 x 39 x 9 = 17900
            这些anchor box通过阀值,去除压边界,极大值抑制(NMS)算法进行筛选,筛选完的框通过在feature_map上获取特征通过 ROI　pooling放入fast-r-cnn网络中得到分类以及框回归(最后的框再用一次　NMS　算法去重复框)

5.YOLOV3(基于darknet-53模型做特征提取)　利用多尺度特征进行对象检测(只用到darknet-53的前52层,最后一层全连接没用到)

    原理:利用三个不同尺度的特征图来进行对象检测(416*416大小图片为例)
    具体实现: 1.卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416*416的话，这里的特征图就是13*13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。
            2.为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。
            3.最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。

            随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。
            YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。
            在COCO数据集这9个先验框是：先验框尺寸是相对原图片(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。

    特征图获取(上采样):   　尺度1: 在基础网络之后添加一些卷积层再输出box信息.
                        尺度2: 从尺度1中的倒数第二层的卷积层上采样(x2)再与最后一个26x26大小的特征图(第二个res8后)相加,再次通过多个卷积后输出box信息.相比尺度1变大两倍.
                        尺度3: 与尺度2类似(与第一个res8后的52*52特征图相加),使用了52x52大小的特征图

    分配方式: 
            1.在最小的13*13特征图上(32倍下采样)（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。
            2.中等的26*26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。
            3.较大的52*52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。
    
    特点:1.3个尺度下都有3个尺寸,所以预测框有 (13*13 + 26*26 + 52*52)*3 个
        2.特征图上的每一个中心网格只返回一个先验框,所以最后返回的只有(13*13 + 26*26 + 52*52)个区域建议框(proposal box)
        3.再通过一部分筛选,最后得到进入计算的框更少

    输出:对于每一个先验框,每一个预测是一个(4+1+80)=85维向量,这个85维向量包含边框坐标(4个数值),边框置信度(1个数值),对象类别的概率(对于COCO数据集，有80种对象).




